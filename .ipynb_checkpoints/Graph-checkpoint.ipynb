{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "\n",
    "# In the real code we'll use argparse instead of namespace, but\n",
    "# namespace lets us set the params from within the notebook\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 4],\n",
       " [2, 3, 4],\n",
       " [-1],\n",
       " [5, 6, 4],\n",
       " [0, 3, 6, 5],\n",
       " [-1],\n",
       " [0, 3],\n",
       " [5, 6, 2],\n",
       " [3, 4],\n",
       " [-1],\n",
       " [0, 6],\n",
       " [0, 5, 1],\n",
       " [5, 6, 2, 1],\n",
       " [0, 1, 5]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = [[5, 4], [2,3,4],[-1], [5, 6, 4], [0, 3, 6, 5], [-1], [0, 3], [5, 6, 2], [3, 4],[-1],[0,6],[0,5,1],[5,6,2,1],[0,1,5]]\n",
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, seqs is a phoney data set with an arbitrary number of patients already, but in the real graph we'll need\n",
    "# to specify the batch size, which will be n_patients.\n",
    "# Also, options won't be a thing in the real code. That'll come from the argparser, and args will be set accordingly\n",
    "options = {'n_patients': 4, 'max_v': 6, 'max_t': 5, 'n_codes': 7, 'code_emb_dim': 4, 'visit_emb_dim': 4,\n",
    "           'log_eps': 1e-6, 'win': 3}\n",
    "args = Namespace(**options)\n",
    "\n",
    "\n",
    "def fill_visit(visit, args):\n",
    "    \"\"\"Fill all deficit visits with -2.\n",
    "    \n",
    "    Ensure that all visits have the same number of ICDs\n",
    "    for efficient tensor logic. If a visit has fewer ICDs,\n",
    "    filler ICDs get one-hot encoded as the zero vector,\n",
    "    so that they affect nothing.\n",
    "    \n",
    "    visit: a list of integer medical codes\n",
    "    \n",
    "    Note: No visit in training or testing should have more\n",
    "    than max_v visits.\n",
    "    \"\"\"\n",
    "    max_v = options['max_v']\n",
    "    if visit != [-1]:\n",
    "        new_visit = []\n",
    "        new_visit.extend(visit)\n",
    "        n_icd = len(visit)\n",
    "        deficit = max_v - n_icd\n",
    "        new_visit.extend([-2] * deficit)\n",
    "        return new_visit\n",
    "    \n",
    "\n",
    "def fill_patient(patient, mask_batch, args):\n",
    "    \"\"\"Ensure that all patients have max_t visits.\n",
    "    \n",
    "    Create visits full of -2s, which are one-hot\n",
    "    encoded as zero vectors. This makes all patients\n",
    "    commensurate for efficient tensor logic.\n",
    "    \n",
    "    patient: list of list of integer codes\n",
    "    max_t: the number of visits all patients ought to have\n",
    "    \n",
    "    Note: No patient in training or test data should have more \n",
    "    than max_t visits.\n",
    "    \"\"\"\n",
    "    max_t = args.max_t\n",
    "    max_v = args.max_v\n",
    "    new_patient = []\n",
    "    new_patient.extend(patient)\n",
    "    new_mask_batch = mask_batch\n",
    "    t = len(new_patient)\n",
    "    deficit = (max_t - t)\n",
    "    new_patient.extend([[-2] * max_v] * deficit)\n",
    "    new_mask_batch.append([[0] * max_v] * deficit)\n",
    "    return new_patient, new_mask_batch, t\n",
    "\n",
    "def tensorize_seqs(seqs, args):\n",
    "    \"\"\"Convert med2vec to tensorflow data.\n",
    "    \n",
    "    seqs: list of list. cf  https://github.com/mp2893/med2vec\n",
    "    \n",
    "    returns:\n",
    "        patients: tensor with shape [patients, max_t, max_v, |C|]\n",
    "        row_masks: numpy array with shape [patients, max_t, max_v]\n",
    "               Later, we will create a [patients, max_t, max_v, |C|]\n",
    "               tensor where the [p, t, i, j] entry is p(c_j|c_i).\n",
    "               Row_masks will drop the rows where c_i is the zero\n",
    "               vector--that is, an NA ICD.\n",
    "               \n",
    "               A separate mask, col_mask, will be created from\n",
    "               patients in order to mask, for each t, those j for\n",
    "               which c_j did not appear in visit t, as well as p(c_i|c_i).\n",
    "               \n",
    "               The masks are to be applied in reverse order of creation.\n",
    "               col_mask is applied with tf.multiply and row_masks\n",
    "               with tf.boolean_mask to avoid needless reshaping.\n",
    "    \"\"\"\n",
    "    max_v = args.max_v\n",
    "    n_codes = args.n_codes\n",
    "    patients = []\n",
    "    new_patient = []\n",
    "    row_masks = []\n",
    "    mask_batch = []\n",
    "    patients_ts = []\n",
    "    for visit in seqs + [[-1]]:\n",
    "        if visit != [-1]:\n",
    "            visit = fill_visit(visit, args)\n",
    "            new_patient.append(visit)\n",
    "        else:\n",
    "            new_patient, mask_batch, t = fill_patient(new_patient,\n",
    "                                                   mask_batch,\n",
    "                                                   args)\n",
    "            patients.append(new_patient)\n",
    "            patients_ts.append(t)\n",
    "            row_masks.append(mask_batch)\n",
    "            new_patient = []\n",
    "            mask_batch = []\n",
    "    patients = np.array(patients)\n",
    "    row_masks = (patients != -2)\n",
    "    patients = tf.one_hot(patients, depth=n_codes)\n",
    "    return patients, row_masks, np.array(patients_ts, dtype=np.float32)\n",
    "\n",
    "def col_masks(patients, args):\n",
    "    \"\"\"Create a mask to cover non-present ICDs.\n",
    "    \n",
    "    For each V_t, for each c_i in V_t,\n",
    "    zero out those p(c_j|c_i) for which c_j is not\n",
    "    in V_t or for which i==j.\n",
    "    \n",
    "    See doc string for tensorize_seqs.\n",
    "    \n",
    "    patients: [patients,max_t,max_v,|C|] tensor\n",
    "    \n",
    "    returns: a binary tensor with shape patients.shape\n",
    "    \"\"\"\n",
    "    max_v = args.max_v\n",
    "    x_t = tf.reduce_sum(patients, axis=-2)\n",
    "    x_t = tf.expand_dims(x_t, -2)\n",
    "    x_t = tf.tile(x_t, [1,1,max_v,1])\n",
    "    col_masks = x_t - patients\n",
    "    return col_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients, row_masks, visit_counts = tensorize_seqs(seqs, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d is the length of the demographic vector, chosen arbitrarily here\n",
    "demo_dim = 6\n",
    "D_t = tf.truncated_normal([args.n_patients, args.max_t, demo_dim], # [patients, max_t, demo_dim]\n",
    "                          mean=0.0,\n",
    "                          stddev=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these constants to Variables for the actual code, but we can't debug on Variables.\n",
    "W_c = tf.truncated_normal([args.code_emb_dim, args.n_codes],\n",
    "                                      mean=0.0,\n",
    "                                      stddev=1.0,\n",
    "                                      #dtype=tf.float32\n",
    "                                      )\n",
    "W_v = tf.truncated_normal([args.visit_emb_dim, args.code_emb_dim + d],\n",
    "                                      mean=0.0,\n",
    "                                      stddev=1.0,\n",
    "                                      #dtype=tf.float32\n",
    "                                      )\n",
    "W_s = tf.truncated_normal([args.n_codes, args.visit_emb_dim],\n",
    "                                      mean=0.0,\n",
    "                                      stddev=1.0,\n",
    "                                      dtype=tf.float32\n",
    "                                      )\n",
    "\n",
    "b_c = tf.zeros([W_c.shape[0], 1], dtype=tf.float32)\n",
    "b_v = tf.zeros([W_v.shape[0], 1], dtype=tf.float32)\n",
    "b_s = tf.zeros([W_s.shape[0], 1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codes_cost(patients, row_masks, visit_counts, W_c=W_c, b_c=b_c, args=args):\n",
    "    \"\"\"Calculate the cost for the code embeddings.\"\"\"\n",
    "    W_c_prime = tf.nn.relu(W_c)\n",
    "    \n",
    "    # tf.matmul doesn't broadcast, and we need to keep these grouped by visit,\n",
    "    # so we need to tile W_c to one copy for every (real or dummy) visit \n",
    "    W_c_tiled = tf.expand_dims(W_c_prime, 0)\n",
    "    W_c_tiled = tf.expand_dims(W_c_tiled, 0)\n",
    "    W_c_tiled = tf.tile(W_c_tiled, [args.n_patients, args.max_t, 1, 1])\n",
    "    \n",
    "    # w_ij is a n_patients X max_t array of code_emb_dim X max_v\n",
    "    # matrices whose columns are the representations of the codes\n",
    "    # appearing in each visit in seqs\n",
    "    w_ij = tf.matmul(W_c_tiled, patients, transpose_b=True)\n",
    "    \n",
    "    # We want a patients X visits X max_v array of\n",
    "    # code_emb_dim X 1 vectors which are the columns\n",
    "    # from w_ij.\n",
    "    w_ij = tf.transpose(w_ij, [0,1,3,2])\n",
    "    \n",
    "    w_ij_shape = [args.n_patients,\n",
    "                  args.max_t,\n",
    "                  args.max_v,\n",
    "                  args.code_emb_dim,\n",
    "                  1]\n",
    "    w_ij = tf.reshape(w_ij, w_ij_shape)\n",
    "    \n",
    "    # tf.multiply will broadcast these columns to\n",
    "    # each column of W_c in each tile of W_c_tiled\n",
    "    pre_sum = tf.multiply(W_c_prime, w_ij)\n",
    "    logits = tf.reduce_sum(pre_sum, -2)\n",
    "    \n",
    "    # Logits now has a n_patients X max_t array of\n",
    "    # max_v X n_codes vectors whose i, jth element\n",
    "    # is the dot product of the code embedding of\n",
    "    # code i (which appears in visit t) with code j\n",
    "    # (which may or may not)\n",
    "    \n",
    "    # The probability of code j given that code i\n",
    "    # is in the same visit\n",
    "    p_j_i = tf.nn.softmax(logits, -1)\n",
    "    \n",
    "    log_p_j_i = tf.log(p_j_i + args.log_eps)\n",
    "    \n",
    "    # Create mask, but don't use it yet. See docstring for col_masks\n",
    "    col_mask = col_masks(patients, args)\n",
    "    \n",
    "    # non_norm because we haven't divided by the number\n",
    "    # of real visits for each patient yet.\n",
    "    non_norm_summands = tf.multiply(log_p_j_i, col_mask)\n",
    "    \n",
    "    # Now for each patient divide by number of real visits of that patient\n",
    "    # Mask rows corresponding to NA ICDs and p_i_i's afterward to ensure \n",
    "    # patient-by-patient division\n",
    "    summands_w_dummies = non_norm_summands / tf.reshape(visit_counts, [args.n_patients,1,1,1])\n",
    "    summands = tf.boolean_mask(summands_w_dummies, row_masks)\n",
    "    codes_cost_per_visit = tf.reduce_sum(summands, -1)\n",
    "    \n",
    "    # Final cost is the batch average per patient of each patient's average\n",
    "    # per visit cost\n",
    "    codes_cost = tf.reduce_mean(codes_cost_per_visit)\n",
    "    return codes_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_ts gets used in predictions and visits cost calculations both, so make them outside of both functions\n",
    "x_ts = tf.reduce_sum(patients, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(x_ts, W_c=W_c, D_t=D_t, W_v=W_v, W_s=W_s, b_c=b_c, b_v=b_v, b_s=b_s, args=args):\n",
    "    \"\"\"Get \\hat{y}_t.\"\"\"\n",
    "    \n",
    "    # We don't need to group by visit in this branch. We also don't need\n",
    "    # to buffer patients with dummy visits.\n",
    "    x_2d = tf.reshape(x_ts, [-1, args.n_codes])\n",
    "    dummy_visit_mask = tf.reshape(tf.minimum(tf.reduce_sum(x_2d, -1), 1), [-1,])\n",
    "    \n",
    "    d_2d = tf.reshape(D_t, [-1, demo_dim])\n",
    "\n",
    "    u_ts = tf.matmul(W_c, x_2d, transpose_b=True)\n",
    "    u_ts = tf.add(u_ts, b_c)\n",
    "    u_ts = tf.transpose(u_ts)\n",
    "    \n",
    "    # In order to store D_t as a tensor it will need to have\n",
    "    # dummy visits just like x_ts does. This also ensures that\n",
    "    # everything aligns correctly when we concatenate, here.\n",
    "    # But after concatenating, we can ditch the dummy visits.\n",
    "    full_vec = tf.concat([u_ts, d_2d], axis=-1)\n",
    "    full_vec = tf.boolean_mask(full_vec, dummy_visit_mask)\n",
    "\n",
    "    v_t = tf.matmul(W_v, full_vec, transpose_b=True)\n",
    "    v_t = tf.add(v_t, b_v)\n",
    "    v_t = tf.transpose(v_t)\n",
    "\n",
    "    pre_soft = tf.matmul(W_s, v_t, transpose_b=True)\n",
    "    pre_soft = tf.add(pre_soft, b_s)\n",
    "    pre_soft = tf.transpose(pre_soft)\n",
    "\n",
    "    y_2d = tf.nn.softmax(pre_soft, axis=-1)\n",
    "    return y_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(7)])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = predictions(x_ts, W_c=W_c, D_t=D_t, W_v=W_v, W_s=W_s, b_c=b_c, b_v=b_v, b_s=b_s, args=args)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.9497531e-15, 3.0782935e-04, 9.9385357e-01, 3.5950020e-14,\n",
       "        5.7701683e-03, 3.1435363e-08, 6.8507281e-05],\n",
       "       [9.2348655e-12, 3.5685066e-06, 9.9999642e-01, 7.4752209e-14,\n",
       "        1.7403806e-08, 6.7259096e-09, 1.7235660e-08],\n",
       "       [2.2023480e-08, 5.6856046e-05, 7.9486763e-01, 1.8992010e-09,\n",
       "        4.7585312e-03, 2.0013741e-01, 1.7955581e-04],\n",
       "       [9.6083152e-01, 3.9936927e-05, 1.0755337e-05, 3.8924474e-02,\n",
       "        2.2750318e-07, 1.4017060e-06, 1.9164002e-04],\n",
       "       [2.1122577e-09, 2.5638387e-01, 4.4405810e-05, 2.5654574e-06,\n",
       "        1.7880700e-03, 8.9333620e-17, 7.4178112e-01],\n",
       "       [6.0368932e-10, 5.2337565e-08, 7.0225001e-14, 3.5436635e-04,\n",
       "        3.9469719e-02, 9.6017587e-01, 1.6058262e-10],\n",
       "       [6.8261201e-05, 1.2205653e-07, 9.9992919e-01, 2.3505806e-10,\n",
       "        3.5330489e-11, 2.1624685e-06, 2.3594173e-07],\n",
       "       [7.7571702e-01, 2.5219366e-05, 2.4376516e-07, 2.4683923e-02,\n",
       "        9.3486534e-09, 1.4845673e-13, 1.9957353e-01],\n",
       "       [4.2849928e-03, 2.2060932e-03, 4.7185808e-06, 8.2789594e-04,\n",
       "        3.9582595e-09, 2.4882348e-19, 9.9267632e-01],\n",
       "       [9.1249640e-06, 5.3081254e-05, 9.8893833e-01, 1.2962893e-09,\n",
       "        2.1093813e-09, 4.4590403e-13, 1.0999469e-02],\n",
       "       [2.7639710e-11, 1.3064015e-12, 1.0000000e+00, 1.0110256e-19,\n",
       "        1.7277181e-17, 4.3004220e-12, 2.3324211e-11]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One row for every real visit, as expected.\n",
    "y_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_ops(win_start, total):\n",
    "    \"\"\"Slide window function.\n",
    "    \n",
    "    Add x_ts from surrounding visits together before\n",
    "    taking the dot product with log(\\hat{y}).\n",
    "    \n",
    "    For passing to tf.while_loop\n",
    "    \"\"\"\n",
    "    summand = tf.slice(x_double_pad, [win_start, 0], normed_x_pad_2d.shape)\n",
    "    return (win_start - 1, tf.add(total, summand))\n",
    "\n",
    "def visits_cost(x_ts, y_2d, visit_counts, args):\n",
    "    \"\"\"Calculate the visits cost.\"\"\"\n",
    "    \n",
    "    # We'll add the x vectors within the window before taking the dot\n",
    "    # product with \\hat{y}_t. To do this, we need to use a sliding\n",
    "    # window, and to make sure patients' sums don't gather terms\n",
    "    # from other patients, we need to pad each patient\n",
    "    x_pad = tf.pad(x_ts, [[0,0], [args.win, args.win], [0, 0]])\n",
    "    \n",
    "    # Note that this is a different mask than the one produced in predictions.\n",
    "    visit_mask = tf.minimum(tf.reduce_sum(x_pad, -1), 1)\n",
    "    visit_mask = tf.reshape(visit_mask, [-1,])\n",
    "    \n",
    "    # We need to flatten x_pad to do the window function, so divide each x\n",
    "    # by the number of visits of that patient *first*.\n",
    "    normed_x_pad = x_pad / tf.reshape(visit_counts, [args.n_patients, 1, 1])\n",
    "    \n",
    "    normed_x_pad_2d = tf.reshape(normed_x_pad, [-1, args.n_codes])\n",
    "    \n",
    "    # Before we padded around each patient. Now pad around the entire list of visits\n",
    "    x_double_pad = tf.pad(normed_x_pad_2d, [[args.win, args.win], [0, 0]])\n",
    "    \n",
    "    def loop_ops(win_start, total):\n",
    "        \"\"\"Slide window function.\n",
    "\n",
    "        Add x_ts from surrounding visits together before\n",
    "        taking the dot product with log(\\hat{y}).\n",
    "\n",
    "        For passing to tf.while_loop\n",
    "        \"\"\"\n",
    "        summand = tf.slice(x_double_pad, [win_start, 0], normed_x_pad_2d.shape)\n",
    "        return (win_start - 1, tf.add(total, summand))\n",
    "    \n",
    "    win_start = 2 * args.win\n",
    "    total = tf.zeros(normed_x_pad_2d.shape)\n",
    "    loop_cond = lambda win_start, total: tf.less(-1, win_start)\n",
    "    loop_fn = lambda win_start, total: loop_ops(win_start, total)\n",
    "    _, window_x_total = tf.while_loop(loop_cond, loop_ops, (win_start, total))\n",
    "    \n",
    "    # Subtract out x_{t+0}\n",
    "    correct_x_totals_pad = tf.subtract(window_x_total, normed_x_pad_2d)\n",
    "    final_x_total = tf.boolean_mask(correct_x_totals_pad, visit_mask)\n",
    "    summands = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
