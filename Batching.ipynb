{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {'seqs_file': 4,\n",
    "           'max_v': 6,\n",
    "           'max_t': 5,\n",
    "           'n_codes': 7,\n",
    "           'n_labels': 4,\n",
    "           'seqs_file': 'seqs',\n",
    "           'labels_file': 'labs',\n",
    "           'demo_file': 'demo',\n",
    "           'out_file': 'zipped_TFR'}\n",
    "args = Namespace(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args=args):\n",
    "    \"\"\"Replace later with dataset stuff.\"\"\"\n",
    "    seqs_file = args.seqs_file\n",
    "    if args.labels_file is not None:\n",
    "        labels_file = args.labels_file\n",
    "    seqs = pickle.load(open(seqs_file, 'rb'))\n",
    "    labs = None\n",
    "    if args.labels_file is not None:\n",
    "        labels_file = args.labels_file\n",
    "        labs = pickle.load(open(labels_file, 'rb'))\n",
    "    D_t = None\n",
    "    demo_dim = 0\n",
    "    if args.demo_file is not None:\n",
    "        demo_file = args.demo_file\n",
    "        D_t = pickle.load(open(demo_file, 'rb'))\n",
    "        demo_dim = D_t.shape[-1]\n",
    "        demo = tf.constant(D_t, dtype=tf.float32)\n",
    "    return seqs, labs, demo, demo_dim\n",
    "\n",
    "\n",
    "def fill_visit(visit, args=args):\n",
    "    \"\"\"Fill all deficit visits with -2.\n",
    "\n",
    "    Ensure that all visits have the same number of ICDs for efficient\n",
    "    tensor logic. If a visit has fewer ICDs, filler ICDs get one-hot\n",
    "    encoded as the zero vector, so that they affect nothing.\n",
    "\n",
    "    visit: a list of integer medical codes\n",
    "\n",
    "    Note: No visit in training or testing should have more than max_v\n",
    "          visits.\n",
    "    \"\"\"\n",
    "    max_v = args.max_v\n",
    "    if visit != [-1]:\n",
    "        new_visit = []\n",
    "        new_visit.extend(visit)\n",
    "        n_icd = len(visit)\n",
    "        deficit = max_v - n_icd\n",
    "        new_visit.extend([-2] * deficit)\n",
    "        return new_visit\n",
    "\n",
    "\n",
    "def fill_patient(patient, mask_batch, args=args):\n",
    "    \"\"\"Ensure that all patients have max_t visits.\n",
    "\n",
    "    Create visits full of -2s, which are one-hot encoded as zero\n",
    "    vectors. This makes all patients commensurate for efficient tensor\n",
    "    logic.\n",
    "\n",
    "    patient: list of list of integer codes\n",
    "    max_t: the number of visits all patients ought to have\n",
    "\n",
    "    Note: No patient in training or test data should have more\n",
    "          than max_t visits.\n",
    "    \"\"\"\n",
    "    max_t = args.max_t\n",
    "    max_v = args.max_v\n",
    "    new_patient = []\n",
    "    new_patient.extend(patient)\n",
    "    new_mask_batch = mask_batch\n",
    "    t = len(new_patient)\n",
    "    deficit = (max_t - t)\n",
    "    new_patient.extend([[-2] * max_v] * deficit)\n",
    "    new_mask_batch.append([[0] * max_v] * deficit)\n",
    "    return new_patient, new_mask_batch, t\n",
    "\n",
    "\n",
    "def tensorize_seqs(seqs, args=args, true_seqs=True):\n",
    "    \"\"\"Convert med2vec to tensorflow data.\n",
    "\n",
    "    seqs: list of list. cf  https://github.com/mp2893/med2vec\n",
    "    true_seqs: bool. Are we tensorizing the true sequences? If false,\n",
    "               we are tonsorizing labels.\n",
    "    returns:\n",
    "        patients: tensor with shape [patients, max_t, max_v, |C|]\n",
    "                  or [patients, max_t, max_v, n_labels] if true_seqs is\n",
    "                  False.\n",
    "        row_masks: numpy array with shape [patients, max_t, max_v]\n",
    "               Later, we will create a [patients, max_t, max_v, |C|]\n",
    "               tensor where the [p, t, i, j] entry is p(c_j|c_i).\n",
    "               Row_masks will drop the rows where c_i is the zero\n",
    "               vector--that is, an NA ICD.\n",
    "\n",
    "               A separate mask, col_mask, will be created from\n",
    "               patients in order to mask, for each t, those j for\n",
    "               which c_j did not appear in visit t, as well as\n",
    "               p(c_i|c_i).\n",
    "\n",
    "               The masks are to be applied in reverse order of creation.\n",
    "               col_mask is applied with tf.multiply and row_masks\n",
    "               with tf.boolean_mask to avoid needless reshaping.\n",
    "        patients_ts: numpy array with shape [patients,] containing the\n",
    "                     number of true visits for each patient.\n",
    "    \"\"\"\n",
    "    patients = []\n",
    "    new_patient = []\n",
    "    row_masks = []\n",
    "    mask_batch = []\n",
    "    patients_ts = []\n",
    "    for visit in seqs + [[-1]]:\n",
    "        if visit != [-1]:\n",
    "            visit = fill_visit(visit, args)\n",
    "            new_patient.append(visit)\n",
    "        else:\n",
    "            new_patient, mask_batch, t = fill_patient(new_patient,\n",
    "                                                      mask_batch,\n",
    "                                                      args)\n",
    "            patients.append(new_patient)\n",
    "            if true_seqs:\n",
    "                patients_ts.append(t)\n",
    "                row_masks.append(mask_batch)\n",
    "                mask_batch = []\n",
    "            new_patient = []\n",
    "    patients = tf.constant(patients)\n",
    "    patients_ts = tf.constant(patients_ts)\n",
    "    patients_ts = tf.expand_dims(patients_ts, -1)\n",
    "    patients_ts = tf.expand_dims(patients_ts, -1)\n",
    "    row_masks = tf.not_equal(patients, -2)\n",
    "    return patients, row_masks, patients_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def serialize_with_labels(patient, label, demo, row_mask, patient_t):\n",
    "    \"\"\"\n",
    "    Creates a tf.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dictionary mapping the feature name to the tf.Example-compatible\n",
    "    # data type.\n",
    "\n",
    "    feature = {'patients': _bytes_feature(patient),\n",
    "               'labels': _bytes_feature(label),\n",
    "               'demo': _bytes_feature(demo),\n",
    "               'row_mask': _bytes_feature(row_mask),\n",
    "               'patient_t': _bytes_feature(patient_t)}\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "\n",
    "    example_proto = tf.train.Example(features=tf.train\n",
    "                                                .Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "\n",
    "def tf_serialize_w_labels(patient, label, demo, row_mask, patient_t):\n",
    "    \"\"\"Map serialize_with_labels to tf.data.Dataset.\"\"\"\n",
    "    tf_string = tf.py_func(serialize_with_labels,\n",
    "                           (patient, label, demo, row_mask, patient_t),\n",
    "                           tf.string)\n",
    "    return tf.reshape(tf_string, ())\n",
    "\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    # Parse the input tf.Example proto using the dictionary below.\n",
    "\n",
    "    feature_description = {\n",
    "    'patients': tf.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'labels': tf.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'demo': tf.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'row_masks': tf.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'patients_ts': tf.FixedLenFeature([], tf.string, default_value='')\n",
    "    }\n",
    "    return tf.parse_single_example(example_proto, feature_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, labs, demo, demo_dim = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients, row_masks, patients_ts = tensorize_seqs(seqs, true_seqs=True)\n",
    "labels, _, _ = tensorize_seqs(labs, true_seqs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tf.int32, tf.int32, tf.float32, tf.bool, tf.float32)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.output_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_ds = tf.data.Dataset().from_tensor_slices(patients)\n",
    "labels_ds = tf.data.Dataset().from_tensor_slices(labels)\n",
    "demo_ds = tf.data.Dataset().from_tensor_slices(demo)\n",
    "row_masks_ds = tf.data.Dataset().from_tensor_slices(row_masks)\n",
    "patients_ts_ds = tf.data.Dataset().from_tensor_slices(patients_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"output = tf.data.Dataset().from_tensor_slices({\\n    'patients': patients,\\n    'labels': labels,\\n    'demo': demo,\\n    'row_masks': row_masks,\\n    'patients_ts': patients_ts\\n    })\""
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"output = tf.data.Dataset().from_tensor_slices({\n",
    "    'patients': patients,\n",
    "    'labels': labels,\n",
    "    'demo': demo,\n",
    "    'row_masks': row_masks,\n",
    "    'patients_ts': patients_ts\n",
    "    })\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.data.Dataset().from_tensor_slices((patients,\n",
    "    labels,\n",
    "    demo,\n",
    "    row_masks,\n",
    "    patients_ts\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tf.int32, tf.int32, tf.float32, tf.bool, tf.float32)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.output_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(5), Dimension(6)]),\n",
       " TensorShape([Dimension(5), Dimension(6)]),\n",
       " TensorShape([Dimension(5), Dimension(6)]),\n",
       " TensorShape([Dimension(5), Dimension(6)]),\n",
       " TensorShape([Dimension(1), Dimension(1)]))"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_it = output.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 5,  4, -2, -2, -2, -2],\n",
      "       [ 2,  3,  4, -2, -2, -2],\n",
      "       [-2, -2, -2, -2, -2, -2],\n",
      "       [-2, -2, -2, -2, -2, -2],\n",
      "       [-2, -2, -2, -2, -2, -2]]), array([[ 3,  2, -2, -2, -2, -2],\n",
      "       [ 0,  3,  2, -2, -2, -2],\n",
      "       [-2, -2, -2, -2, -2, -2],\n",
      "       [-2, -2, -2, -2, -2, -2],\n",
      "       [-2, -2, -2, -2, -2, -2]]), array([[-1.2766646 , -8.258818  ,  1.0295945 , -9.932409  ,  3.4238288 ,\n",
      "         4.7623577 ],\n",
      "       [ 4.781394  , -2.7135177 ,  8.019953  ,  0.30808517,  0.31495225,\n",
      "        -3.8640842 ],\n",
      "       [ 0.3880088 , -6.512215  ,  3.4989486 ,  2.7263956 , -1.5292437 ,\n",
      "        -0.5698157 ],\n",
      "       [ 4.377598  , -1.8317424 ,  2.9549975 , -1.0864103 ,  4.6243834 ,\n",
      "        -0.11334902],\n",
      "       [-7.137146  ,  0.8752901 , -4.1808968 ,  3.2075074 ,  4.62191   ,\n",
      "        -1.7289466 ]], dtype=float32), array([[ True,  True, False, False, False, False],\n",
      "       [ True,  True,  True, False, False, False],\n",
      "       [False, False, False, False, False, False],\n",
      "       [False, False, False, False, False, False],\n",
      "       [False, False, False, False, False, False]]), array([[2.]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(output_it.get_next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized = output.map(lambda patients, labels, demo, row_masks, patients_ts: (tf.serialize_tensor(patients),\n",
    "                                           tf.serialize_tensor(labels),\n",
    "                                           tf.serialize_tensor(demo),\n",
    "                                           tf.serialize_tensor(row_masks),\n",
    "                                           tf.serialize_tensor(patients_ts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_it = serialized.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'\\x08\\x03\\x12\\x08\\x12\\x02\\x08\\x05\\x12\\x02\\x08\\x06\"x\\x05\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff', b'\\x08\\x03\\x12\\x08\\x12\\x02\\x08\\x05\\x12\\x02\\x08\\x06\"x\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff', b'\\x08\\x01\\x12\\x08\\x12\\x02\\x08\\x05\\x12\\x02\\x08\\x06\"x\\xdb\\xaf&>\\xec\\x1e\\xae?`\\x89\\x05\\xbfCZ\\x9e\\xc0\\nT\\x10\\xc0x\\xd5\\xac@\\x14;z\\xc0\\xcc\\xdd\\xfa\\xc0\\xfc\\xdb\\x1bA\\x08\\x06\\xbd\\xc0t\\xa1\\x0c\\xc1\\xf8\\xdc\\xe0\\xbff\\xfc\\xa2@\\x0c\\xda\\x00?\\xdf\\xc0\\n@\\x11\\x14@?Ip\\x97\\xbf|BX\\xc0?P\\xf1\\xbf&\\xbe\\xaf\\xc0\\xe8a\\x07\\xbfk\\x03\\x9d@n\\xd1\\x9a@\\xb9\\xf78@A\\xb8\\x16A\\xc6\\x17U\\xbf\\xd2:j\\xc0\\xc1\\xf8\\xf0\\xc0i9\\x1d\\xc1j\\x002@', b'\\x08\\n\\x12\\x08\\x12\\x02\\x08\\x05\\x12\\x02\\x08\\x06\"\\x1e\\x01\\x01\\x01\\x00\\x00\\x00\\x01\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00', b'\\x08\\x01\\x12\\x08\\x12\\x02\\x08\\x01\\x12\\x02\\x08\\x01\"\\x04\\x00\\x00\\x00@')\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(serialized_it.get_next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_features_dataset = serialized.map(tf_serialize_w_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.string"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialized_features_dataset.output_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serialized_features_dataset.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfd_it = serialized_features_dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.contrib.data.TFRecordWriter(args.out_file)\n",
    "writeop = writer.write(serialized_features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(writeop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['zipped_TFR']\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_raw = raw_dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n\\xe7\\x05\\n\\xea\\x01\\n\\x08patients\\x12\\xdd\\x01\\n\\xda\\x01\\n\\xd7\\x01\\x08\\x03\\x12\\x08\\x12\\x02\\x08\\x05\\x12\\x02\\x08\\n\"\\xc8\\x01\\x05\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\n\\xe8\\x01\\n\\x06labels\\x12\\xdd\\x01\\n\\xda\\x01\\n\\xd7\\x01\\x08\\x03\\x12\\x08\\x12\\x02\\x08\\x05\\x12\\x02\\x08\\n\"\\xc8\\x01\\x03\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\n\\x95\\x01\\n\\x04demo\\x12\\x8c\\x01\\n\\x89\\x01\\n\\x86\\x01\\x08\\x01\\x12\\x08\\x12\\x02\\x08\\x05\\x12\\x02\\x08\\x06\"x\\xbfi\\xa3\\xbf\\x1e$\\x04\\xc1\\xc1\\xc9\\x83?&\\xeb\\x1e\\xc1\\x03 [@<e\\x98@.\\x01\\x99@F\\xaa-\\xc0\\xbaQ\\x00AW\\xbd\\x9d>lA\\xa1>(Mw\\xc0\\x17\\xa9\\xc6>\\x11d\\xd0\\xc0\\xc6\\xee_@D}.@B\\xbe\\xc3\\xbfq\\xdf\\x11\\xbfH\\x15\\x8c@\\x89v\\xea\\xbf\\xae\\x1e=@~\\x0f\\x8b\\xbf\\xf3\\xfa\\x93@\\x88#\\xe8\\xbd\\x80c\\xe4\\xc0\\x03\\x13`?\\xe8\\xc9\\x85\\xc0\\xcdGM@\\xb0\\xe6\\x93@\\x1fN\\xdd\\xbf\\nP\\n\\x08row_mask\\x12D\\nB\\n@\\x08\\n\\x12\\x08\\x12\\x02\\x08\\x05\\x12\\x02\\x08\\n\"2\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\n#\\n\\tpatient_t\\x12\\x16\\n\\x14\\n\\x12\\x08\\x01\\x12\\x08\\x12\\x02\\x08\\x01\\x12\\x02\\x08\\x01\"\\x04\\x00\\x00\\x00@'\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(it_raw.get_next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'demo': b'\\x08\\x03\\x12\\x08\\x12\\x02\\x08\\x05\\x12\\x02\\x08\\x06\"x\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff', 'labels': b'\\x08\\x01\\x12\\x08\\x12\\x02\\x08\\x05\\x12\\x02\\x08\\x06\"x\\xdb\\xaf&>\\xec\\x1e\\xae?`\\x89\\x05\\xbfCZ\\x9e\\xc0\\nT\\x10\\xc0x\\xd5\\xac@\\x14;z\\xc0\\xcc\\xdd\\xfa\\xc0\\xfc\\xdb\\x1bA\\x08\\x06\\xbd\\xc0t\\xa1\\x0c\\xc1\\xf8\\xdc\\xe0\\xbff\\xfc\\xa2@\\x0c\\xda\\x00?\\xdf\\xc0\\n@\\x11\\x14@?Ip\\x97\\xbf|BX\\xc0?P\\xf1\\xbf&\\xbe\\xaf\\xc0\\xe8a\\x07\\xbfk\\x03\\x9d@n\\xd1\\x9a@\\xb9\\xf78@A\\xb8\\x16A\\xc6\\x17U\\xbf\\xd2:j\\xc0\\xc1\\xf8\\xf0\\xc0i9\\x1d\\xc1j\\x002@', 'patient_ts': b'', 'patients': b'\\x08\\x03\\x12\\x08\\x12\\x02\\x08\\x05\\x12\\x02\\x08\\x06\"x\\x05\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x04\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff\\xfe\\xff\\xff\\xff', 'row_masks': b''}\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(_parse_function(it_raw.get_next())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = raw_dataset.map(_parse_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_it = parsed.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  6  4 -2 -2 -2 -2 -2 -2 -2]\n",
      " [ 0  3  6  5 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2]\n",
      " [-2 -2 -2 -2 -2 -2 -2 -2 -2 -2]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(tf.parse_tensor(parsed_it.get_next()['patients'], out_type=tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
