{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import pickle\n",
    "\n",
    "# In the real code we'll use argparse instead of namespace, but\n",
    "# namespace lets us set the params from within the notebook\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 4],\n",
       " [2, 3, 4],\n",
       " [-1],\n",
       " [5, 6, 4],\n",
       " [0, 3, 6, 5],\n",
       " [-1],\n",
       " [0, 3],\n",
       " [5, 6, 2],\n",
       " [3, 4],\n",
       " [-1],\n",
       " [0, 6],\n",
       " [0, 5, 1],\n",
       " [5, 6, 2, 1],\n",
       " [0, 1, 5]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = [[5, 4], [2,3,4],[-1], [5, 6, 4], [0, 3, 6, 5], [-1], [0, 3], [5, 6, 2], [3, 4],[-1],[0,6],[0,5,1],[5,6,2,1],[0,1,5]]\n",
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, seqs is a phoney data set with an arbitrary number of patients already, but in the real graph we'll need\n",
    "# to specify the batch size, which will be n_patients.\n",
    "# Also, options won't be a thing in the real code. That'll come from the argparser, and args will be set accordingly\n",
    "options = {'n_patients': 4, 'max_v': 6, 'max_t': 5, 'n_codes': 7, 'code_emb_dim': 4, 'visit_emb_dim': 4,\n",
    "           'log_eps': 1e-6, 'win': 3}\n",
    "args = Namespace(**options)\n",
    "\n",
    "\n",
    "def fill_visit(visit, args):\n",
    "    \"\"\"Fill all deficit visits with -2.\n",
    "    \n",
    "    Ensure that all visits have the same number of ICDs\n",
    "    for efficient tensor logic. If a visit has fewer ICDs,\n",
    "    filler ICDs get one-hot encoded as the zero vector,\n",
    "    so that they affect nothing.\n",
    "    \n",
    "    visit: a list of integer medical codes\n",
    "    \n",
    "    Note: No visit in training or testing should have more\n",
    "    than max_v visits.\n",
    "    \"\"\"\n",
    "    max_v = options['max_v']\n",
    "    if visit != [-1]:\n",
    "        new_visit = []\n",
    "        new_visit.extend(visit)\n",
    "        n_icd = len(visit)\n",
    "        deficit = max_v - n_icd\n",
    "        new_visit.extend([-2] * deficit)\n",
    "        return new_visit\n",
    "    \n",
    "\n",
    "def fill_patient(patient, mask_batch, args):\n",
    "    \"\"\"Ensure that all patients have max_t visits.\n",
    "    \n",
    "    Create visits full of -2s, which are one-hot\n",
    "    encoded as zero vectors. This makes all patients\n",
    "    commensurate for efficient tensor logic.\n",
    "    \n",
    "    patient: list of list of integer codes\n",
    "    max_t: the number of visits all patients ought to have\n",
    "    \n",
    "    Note: No patient in training or test data should have more \n",
    "    than max_t visits.\n",
    "    \"\"\"\n",
    "    max_t = args.max_t\n",
    "    max_v = args.max_v\n",
    "    new_patient = []\n",
    "    new_patient.extend(patient)\n",
    "    new_mask_batch = mask_batch\n",
    "    t = len(new_patient)\n",
    "    deficit = (max_t - t)\n",
    "    new_patient.extend([[-2] * max_v] * deficit)\n",
    "    new_mask_batch.append([[0] * max_v] * deficit)\n",
    "    return new_patient, new_mask_batch, t\n",
    "\n",
    "def tensorize_seqs(seqs, args):\n",
    "    \"\"\"Convert med2vec to tensorflow data.\n",
    "    \n",
    "    seqs: list of list. cf  https://github.com/mp2893/med2vec\n",
    "    \n",
    "    returns:\n",
    "        patients: tensor with shape [patients, max_t, max_v, |C|]\n",
    "        row_masks: numpy array with shape [patients, max_t, max_v]\n",
    "               Later, we will create a [patients, max_t, max_v, |C|]\n",
    "               tensor where the [p, t, i, j] entry is p(c_j|c_i).\n",
    "               Row_masks will drop the rows where c_i is the zero\n",
    "               vector--that is, an NA ICD.\n",
    "               \n",
    "               A separate mask, col_mask, will be created from\n",
    "               patients in order to mask, for each t, those j for\n",
    "               which c_j did not appear in visit t, as well as p(c_i|c_i).\n",
    "               \n",
    "               The masks are to be applied in reverse order of creation.\n",
    "               col_mask is applied with tf.multiply and row_masks\n",
    "               with tf.boolean_mask to avoid needless reshaping.\n",
    "    \"\"\"\n",
    "    max_v = args.max_v\n",
    "    n_codes = args.n_codes\n",
    "    patients = []\n",
    "    new_patient = []\n",
    "    row_masks = []\n",
    "    mask_batch = []\n",
    "    patients_ts = []\n",
    "    for visit in seqs + [[-1]]:\n",
    "        if visit != [-1]:\n",
    "            visit = fill_visit(visit, args)\n",
    "            new_patient.append(visit)\n",
    "        else:\n",
    "            new_patient, mask_batch, t = fill_patient(new_patient,\n",
    "                                                   mask_batch,\n",
    "                                                   args)\n",
    "            patients.append(new_patient)\n",
    "            patients_ts.append(t)\n",
    "            row_masks.append(mask_batch)\n",
    "            new_patient = []\n",
    "            mask_batch = []\n",
    "    patients = np.array(patients)\n",
    "    row_masks = (patients != -2)\n",
    "    patients = tf.one_hot(patients, depth=n_codes)\n",
    "    return patients, row_masks, np.array(patients_ts, dtype=np.float32)\n",
    "\n",
    "def col_masks(patients, args):\n",
    "    \"\"\"Create a mask to cover non-present ICDs.\n",
    "    \n",
    "    For each V_t, for each c_i in V_t,\n",
    "    zero out those p(c_j|c_i) for which c_j is not\n",
    "    in V_t or for which i==j.\n",
    "    \n",
    "    See doc string for tensorize_seqs.\n",
    "    \n",
    "    patients: [patients,max_t,max_v,|C|] tensor\n",
    "    \n",
    "    returns: a binary tensor with shape patients.shape\n",
    "    \"\"\"\n",
    "    max_v = args.max_v\n",
    "    x_t = tf.reduce_sum(patients, axis=-2)\n",
    "    x_t = tf.expand_dims(x_t, -2)\n",
    "    x_t = tf.tile(x_t, [1,1,max_v,1])\n",
    "    col_masks = x_t - patients\n",
    "    return col_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients, row_masks, visit_counts = tensorize_seqs(seqs, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d is the length of the demographic vector, chosen arbitrarily here\n",
    "demo_dim = 6\n",
    "D_t = tf.truncated_normal([args.n_patients, args.max_t, demo_dim], # [patients, max_t, demo_dim]\n",
    "                          mean=0.0,\n",
    "                          stddev=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these constants to Variables for the actual code, but we can't debug on Variables.\n",
    "W_c = tf.truncated_normal([args.code_emb_dim, args.n_codes],\n",
    "                                      mean=0.0,\n",
    "                                      stddev=1.0,\n",
    "                                      #dtype=tf.float32\n",
    "                                      )\n",
    "W_v = tf.truncated_normal([args.visit_emb_dim, args.code_emb_dim + d],\n",
    "                                      mean=0.0,\n",
    "                                      stddev=1.0,\n",
    "                                      #dtype=tf.float32\n",
    "                                      )\n",
    "W_s = tf.truncated_normal([args.n_codes, args.visit_emb_dim],\n",
    "                                      mean=0.0,\n",
    "                                      stddev=1.0,\n",
    "                                      dtype=tf.float32\n",
    "                                      )\n",
    "\n",
    "b_c = tf.zeros([W_c.shape[0], 1], dtype=tf.float32)\n",
    "b_v = tf.zeros([W_v.shape[0], 1], dtype=tf.float32)\n",
    "b_s = tf.zeros([W_s.shape[0], 1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codes_cost(patients, row_masks, visit_counts, W_c=W_c, b_c=b_c, args=args):\n",
    "    \"\"\"Calculate the cost for the code embeddings.\"\"\"\n",
    "    W_c_prime = tf.nn.relu(W_c)\n",
    "    \n",
    "    # tf.matmul doesn't broadcast, and we need to keep these grouped by visit,\n",
    "    # so we need to tile W_c to one copy for every (real or dummy) visit \n",
    "    W_c_tiled = tf.expand_dims(W_c_prime, 0)\n",
    "    W_c_tiled = tf.expand_dims(W_c_tiled, 0)\n",
    "    W_c_tiled = tf.tile(W_c_tiled, [args.n_patients, args.max_t, 1, 1])\n",
    "    \n",
    "    # w_ij is a n_patients X max_t array of code_emb_dim X max_v\n",
    "    # matrices whose columns are the representations of the codes\n",
    "    # appearing in each visit in seqs\n",
    "    w_ij = tf.matmul(W_c_tiled, patients, transpose_b=True)\n",
    "    \n",
    "    # We want a patients X visits X max_v array of\n",
    "    # code_emb_dim X 1 vectors which are the columns\n",
    "    # from w_ij.\n",
    "    w_ij = tf.transpose(w_ij, [0,1,3,2])\n",
    "    \n",
    "    w_ij_shape = [args.n_patients,\n",
    "                  args.max_t,\n",
    "                  args.max_v,\n",
    "                  args.code_emb_dim,\n",
    "                  1]\n",
    "    w_ij = tf.reshape(w_ij, w_ij_shape)\n",
    "    \n",
    "    # tf.multiply will broadcast these columns to\n",
    "    # each column of W_c in each tile of W_c_tiled\n",
    "    pre_sum = tf.multiply(W_c_prime, w_ij)\n",
    "    logits = tf.reduce_sum(pre_sum, -2)\n",
    "    \n",
    "    # Logits now has a n_patients X max_t array of\n",
    "    # max_v X n_codes vectors whose i, jth element\n",
    "    # is the dot product of the code embedding of\n",
    "    # code i (which appears in visit t) with code j\n",
    "    # (which may or may not)\n",
    "    \n",
    "    # The probability of code j given that code i\n",
    "    # is in the same visit\n",
    "    p_j_i = tf.nn.softmax(logits, -1)\n",
    "    \n",
    "    log_p_j_i = tf.log(p_j_i + args.log_eps)\n",
    "    \n",
    "    # Create mask, but don't use it yet. See docstring for col_masks\n",
    "    col_mask = col_masks(patients, args)\n",
    "    \n",
    "    # non_norm because we haven't divided by the number\n",
    "    # of real visits for each patient yet.\n",
    "    non_norm_summands = tf.multiply(log_p_j_i, col_mask)\n",
    "    \n",
    "    # Now for each patient divide by number of real visits of that patient\n",
    "    # Mask rows corresponding to NA ICDs and p_i_i's afterward to ensure \n",
    "    # patient-by-patient division\n",
    "    summands_w_dummies = non_norm_summands / tf.reshape(visit_counts, [args.n_patients,1,1,1])\n",
    "    summands = tf.boolean_mask(summands_w_dummies, row_masks)\n",
    "    codes_cost_per_visit = tf.reduce_sum(summands, -1)\n",
    "    \n",
    "    # Final cost is the batch average per patient of each patient's average\n",
    "    # per visit cost\n",
    "    codes_cost = tf.reduce_mean(codes_cost_per_visit)\n",
    "    return codes_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_ts gets used in predictions and visits cost calculations both, so make them outside of both functions\n",
    "x_ts = tf.reduce_sum(pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(x_ts, W_c=W_c, D_t=D_t, W_v=W_v, W_s=W_s, b_c=b_c, b_v=b_v, b_s=b_s, args=args):\n",
    "    \"\"\"Get \\hat{y}_t.\"\"\"\n",
    "    \n",
    "    # We don't need to group by visit in this branch. We also don't need\n",
    "    # to buffer patients with dummy visits.\n",
    "    x_2d = tf.reshape(x_ts, [-1, args.n_codes])\n",
    "    dummy_visit_mask = tf.reshape(tf.minimum(tf.reduce_sum(x_2d, -1), 1), [-1,])\n",
    "    \n",
    "    d_2d = tf.reshape(D_t, [-1, demo_dim])\n",
    "\n",
    "    u_ts = tf.matmul(W_c, x_2d, transpose_b=True)\n",
    "    u_ts = tf.add(u_ts, b_c)\n",
    "    u_ts = tf.transpose(u_ts)\n",
    "    \n",
    "    # In order to store D_t as a tensor it will need to have\n",
    "    # dummy visits just like x_ts does. This also ensures that\n",
    "    # everything aligns correctly when we concatenate, here.\n",
    "    # But after concatenating, we can ditch the dummy visits.\n",
    "    full_vec = tf.concat([u_ts, d_2d], axis=-1)\n",
    "    full_vec = tf.boolean_mask(full_vec, dummy_visit_mask)\n",
    "\n",
    "    v_t = tf.matmul(W_v, full_vec, transpose_b=True)\n",
    "    v_t = tf.add(v_t, b_v)\n",
    "    v_t = tf.transpose(v_t)\n",
    "\n",
    "    pre_soft = tf.matmul(W_s, v_t, transpose_b=True)\n",
    "    pre_soft = tf.add(pre_soft, b_s)\n",
    "    pre_soft = tf.transpose(pre_soft)\n",
    "\n",
    "    y_2d = tf.nn.softmax(pre_soft, axis=-1)\n",
    "    return y_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax_19:0' shape=(?, 7) dtype=float32>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_2d = predictions(x_ts, W_c=W_c, D_t=D_t, W_v=W_v, W_s=W_s, b_c=b_c, b_v=b_v, b_s=b_s, args=args)\n",
    "y_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.6085853e-18, 9.0658553e-08, 4.4319490e-09, 3.5209838e-02,\n",
       "        9.6479005e-01, 1.0396769e-26, 2.0756780e-22],\n",
       "       [6.9340423e-08, 6.5690224e-17, 1.8994191e-30, 2.2932563e-12,\n",
       "        5.3013420e-32, 9.9999988e-01, 2.2513388e-19],\n",
       "       [1.2073571e-26, 3.6456187e-16, 1.6760706e-03, 3.2039849e-15,\n",
       "        9.9832386e-01, 2.3809569e-35, 2.4894056e-18],\n",
       "       [6.4953824e-08, 2.7575591e-17, 4.9990022e-21, 1.6656639e-14,\n",
       "        3.2074747e-25, 9.9999988e-01, 7.8157717e-13],\n",
       "       [3.5257922e-15, 1.3173596e-33, 3.2943777e-33, 1.4652709e-26,\n",
       "        0.0000000e+00, 1.0000000e+00, 8.6632166e-19],\n",
       "       [8.6561788e-04, 7.5667504e-07, 9.6243136e-15, 1.0567531e-08,\n",
       "        2.7946128e-15, 9.9913329e-01, 4.0201925e-07],\n",
       "       [3.1391869e-04, 5.1624817e-03, 4.7404148e-02, 7.8861404e-04,\n",
       "        9.4576669e-01, 1.9582774e-05, 5.4457737e-04],\n",
       "       [0.0000000e+00, 4.2276565e-20, 5.2035183e-13, 2.9162840e-27,\n",
       "        1.0000000e+00, 0.0000000e+00, 2.3846293e-33],\n",
       "       [2.5320991e-26, 1.1534347e-11, 3.6836536e-12, 6.6066169e-25,\n",
       "        1.0000000e+00, 8.8419545e-38, 1.9410995e-21],\n",
       "       [4.7128964e-11, 6.8158140e-10, 7.4117923e-01, 2.5134712e-01,\n",
       "        7.4736713e-03, 9.1133188e-13, 2.4616562e-09],\n",
       "       [1.7677435e-26, 2.6886816e-16, 2.9020564e-04, 4.6135899e-12,\n",
       "        9.9970978e-01, 2.0562724e-35, 5.7814052e-21]], dtype=float32)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One row for every real visit, as expected.\n",
    "y_2d.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_ops(win_start, total):\n",
    "    \"\"\"Slide window function.\n",
    "    \n",
    "    Add x_ts from surrounding visits together before\n",
    "    taking the dot product with log(\\hat{y}).\n",
    "    \n",
    "    For passing to tf.while_loop\n",
    "    \"\"\"\n",
    "    summand = tf.slice(x_double_pad, [win_start, 0], normed_x_pad_2d.shape)\n",
    "    return (win_start - 1, tf.add(total, summand))\n",
    "\n",
    "def visits_cost(x_ts, y_2d, visit_counts, args):\n",
    "    \"\"\"Calculate the visits cost.\"\"\"\n",
    "    \n",
    "    # We'll add the x vectors within the window before taking the dot\n",
    "    # product with \\hat{y}_t. To do this, we need to use a sliding\n",
    "    # window, and to make sure patients' sums don't gather terms\n",
    "    # from other patients, we need to pad each patient\n",
    "    x_pad = tf.pad(x_ts, [[0,0], [args.win, args.win], [0, 0]])\n",
    "    \n",
    "    # Because different \\hat{y}_t have different numbers of\n",
    "    # neighboring x_t in their window, we can't really avoid passing\n",
    "    # 1-x_ts through the same loop as x_ts by subtracting final_x_totals\n",
    "    # from 2*win / visit_counts, say\n",
    "    z_pad = 1. - x_pad\n",
    "    \n",
    "    # Note that this is a different mask than the one produced in predictions.\n",
    "    visit_mask = tf.minimum(tf.reduce_sum(x_pad, -1), 1)\n",
    "    visit_mask = tf.reshape(visit_mask, [-1,])\n",
    "    \n",
    "    # We need to flatten x_pad to do the window function, so divide each x\n",
    "    # by the number of visits of that patient *first*.\n",
    "    normed_x_pad = x_pad / tf.reshape(visit_counts, [args.n_patients, 1, 1])\n",
    "    normed_z_pad = z_pad / tf.reshape(visit_counts, [args.n_patients, 1, 1])\n",
    "    \n",
    "    normed_x_pad_2d = tf.reshape(normed_x_pad, [-1, args.n_codes])\n",
    "    normed_z_pad_2d = tf.reshape(normed_z_pad, [-1, args.n_codes])\n",
    "    \n",
    "    # Before we padded around each patient. Now pad around the entire list of visits\n",
    "    x_double_pad = tf.pad(normed_x_pad_2d, [[args.win, args.win], [0, 0]])\n",
    "    z_double_pad = tf.pad(normed_z_pad_2d, [[args.win, args.win], [0, 0]])\n",
    "    \n",
    "    def loop_ops(win_start, totalx, totalz):\n",
    "        \"\"\"Slide window function.\n",
    "\n",
    "        Add x_ts from surrounding visits together before\n",
    "        taking the dot product with log(\\hat{y}).\n",
    "\n",
    "        For passing to tf.while_loop\n",
    "        \"\"\"\n",
    "        summandx = tf.slice(x_double_pad, [win_start, 0], normed_x_pad_2d.shape)\n",
    "        summandz = tf.slice(z_double_pad, [win_start, 0], normed_z_pad_2d.shape)\n",
    "        return (win_start - 1, tf.add(totalx, summandx), tf.add(totalz, summandz))\n",
    "    \n",
    "    win_start = 2 * args.win\n",
    "    totalx = tf.zeros(normed_x_pad_2d.shape)\n",
    "    totalz = tf.zeros(normed_z_pad_2d.shape)\n",
    "    loop_cond = lambda win_start, totalx, totalz: tf.less(-1, win_start)\n",
    "    loop_fn = lambda win_start, totalx, totalz: loop_ops(win_start, totalx, totalz)\n",
    "    _, window_x_total, window_z_total = tf.while_loop(loop_cond, loop_ops, (win_start, totalx, totalz))\n",
    "\n",
    "    # Subtract out x_{t+0}\n",
    "    correct_x_totals_pad = tf.subtract(window_x_total, normed_x_pad_2d)\n",
    "    correct_z_totals_pad = tf.subtract(window_z_total, normed_z_pad_2d)\n",
    "    \n",
    "    final_x_total = tf.boolean_mask(correct_x_totals_pad, visit_mask)\n",
    "    final_z_total = tf.boolean_mask(correct_z_totals_pad, visit_mask)\n",
    "\n",
    "    summandsx = tf.multiply(final_x_total, tf.log(y_2d + args.log_eps))\n",
    "    summandsz = tf.multiply(final_z_total, tf.log(1. - y_2d + args.log_eps))\n",
    "    \n",
    "    sumx = tf.reduce_sum(summandsx)\n",
    "    sumz = tf.reduce_sum(summandsz)\n",
    "\n",
    "    visits_cost = tf.subtract(sumz, sumx)\n",
    "    return visits_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-4.468975  ,  0.23625278, -5.0017333 ,  6.235909  ,\n",
       "          7.915654  ,  5.0930014 ],\n",
       "        [-1.3927264 ,  2.830865  , -5.4919147 ,  7.143624  ,\n",
       "         -1.1168783 ,  6.765215  ],\n",
       "        [-1.8450476 ,  5.873818  , -0.3785586 ,  2.9186127 ,\n",
       "          2.6538143 , -2.3643394 ],\n",
       "        [ 4.080118  , -2.243655  ,  4.24465   , -3.3345423 ,\n",
       "          5.734318  ,  2.8201718 ],\n",
       "        [ 6.5554004 , -2.7130067 ,  4.681351  , -0.59912705,\n",
       "          1.648201  ,  3.964186  ]],\n",
       "\n",
       "       [[-2.48018   , -2.193721  ,  9.169472  ,  2.6991568 ,\n",
       "          4.9216933 , -0.7434596 ],\n",
       "        [ 3.7783136 , -0.4528087 ,  0.96824914,  0.4719281 ,\n",
       "          0.0434884 , -1.6019058 ],\n",
       "        [-3.5891201 ,  5.016054  ,  1.9721509 , -0.7044741 ,\n",
       "          6.6981606 , -2.4262743 ],\n",
       "        [ 2.2948596 ,  1.4530307 ,  2.363258  ,  0.5358587 ,\n",
       "         -0.42536604,  2.8189712 ],\n",
       "        [-2.6930475 ,  8.998437  , -3.9350052 ,  0.72095776,\n",
       "          5.6756454 , -0.17493442]],\n",
       "\n",
       "       [[-6.3981533 , -6.186591  ,  7.5065136 ,  8.06219   ,\n",
       "          1.657533  ,  2.0831127 ],\n",
       "        [ 1.4072309 , -0.58818024,  2.8620024 , -2.7354167 ,\n",
       "          2.7492027 , -4.2100344 ],\n",
       "        [-1.0102239 ,  8.2207365 , -8.092362  ,  1.8371062 ,\n",
       "          3.616145  , -2.2806487 ],\n",
       "        [ 4.361847  , -0.9783593 ,  4.8838506 , -1.7529517 ,\n",
       "          3.8973095 ,  0.76465493],\n",
       "        [ 1.602663  ,  2.0494545 ,  3.7661076 , -3.9500399 ,\n",
       "         -0.04100538,  5.8034296 ]],\n",
       "\n",
       "       [[ 2.3533878 , -6.8077755 , -0.21154366, -8.714668  ,\n",
       "          4.571504  ,  3.640511  ],\n",
       "        [ 4.393288  ,  5.356288  ,  3.820226  , -2.0928054 ,\n",
       "         -0.32516184, -2.92032   ],\n",
       "        [ 4.875923  , -2.0606663 , -0.6972986 , -5.292148  ,\n",
       "          3.9434447 , -1.3594558 ],\n",
       "        [ 5.178929  ,  2.3796134 , -2.2462792 ,  4.0080066 ,\n",
       "          4.0672307 , -4.2395344 ],\n",
       "        [ 8.092671  ,  5.049959  , -8.576732  , -3.9621243 ,\n",
       "         -4.8539095 , -4.9273357 ]]], dtype=float32)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_t.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('seqs', 'wb') as seqs_file:\n",
    "    pickle.dump(seqs, seqs_file, protocol=2)\n",
    "    \n",
    "with open('demo', 'wb') as demo_file:\n",
    "    pickle.dump(D_t.eval(), demo_file, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(4), Dimension(5), Dimension(6), Dimension(7)])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 4],\n",
       " [2, 3, 4],\n",
       " [-1],\n",
       " [5, 6, 4],\n",
       " [0, 3, 6, 5],\n",
       " [-1],\n",
       " [0, 3],\n",
       " [5, 6, 2],\n",
       " [3, 4],\n",
       " [-1],\n",
       " [0, 6],\n",
       " [0, 5, 1],\n",
       " [5, 6, 2, 1],\n",
       " [0, 1, 5]]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_labels = 4\n",
    "labs = [[3, 2],\n",
    " [0, 3, 2],\n",
    " [-1],\n",
    " [3, 3, 2],\n",
    " [0, 3, 3, 2],\n",
    " [-1],\n",
    " [0, 2],\n",
    " [3, 2, 0],\n",
    " [1, 2],\n",
    " [-1],\n",
    " [0, 1],\n",
    " [0, 3, 0],\n",
    " [2, 2, 0, 1],\n",
    " [0, 1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('labs', 'wb') as labs_file:\n",
    "    pickle.dump(labs, labs_file, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
